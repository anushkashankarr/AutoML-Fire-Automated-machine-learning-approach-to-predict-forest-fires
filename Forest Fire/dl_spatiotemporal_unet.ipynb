{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AutoML-Fire: Spatiotemporal U-Net CNN (Notebook-first)\n",
        "\n",
        "This notebook implements a simple spatiotemporal CNN baseline for fire prediction using a U-Net architecture. The model takes the last T days of meteorological data as input channels and predicts next-day fire counts on a 0.25Â° grid.\n",
        "\n",
        "**Key Features:**\n",
        "- Spatiotemporal CNN with U-Net-like architecture\n",
        "- Input: Last T days of meteorological variables as channels\n",
        "- Output: Next-day fire count prediction per grid cell\n",
        "- Handles missing data with masking\n",
        "- Supports both static and dynamic features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Configuration for Spatiotemporal U-Net CNN\"\"\"\n",
        "    # Data paths\n",
        "    data_zarr_or_nc_path: str = \"data/spatiotemporal_data.zarr\"  # Placeholder - update with your path\n",
        "    \n",
        "    # Model parameters\n",
        "    T: int = 7  # Number of time steps to look back\n",
        "    seed: int = 42\n",
        "    \n",
        "    # Training/validation date ranges\n",
        "    train_start: str = \"2020-01-01\"\n",
        "    train_end: str = \"2022-12-31\"\n",
        "    val_start: str = \"2023-01-01\"\n",
        "    val_end: str = \"2023-12-31\"\n",
        "    \n",
        "    # Variables\n",
        "    variables: List[str] = [\"tmin\", \"tmax\", \"humidity\", \"windspeed\", \"soil_moisture\", \"ndvi\", \"rain\", \"cloudcover\"]\n",
        "    static_variables: List[str] = [\"elevation\", \"slope\", \"aspect\", \"landcover\"]  # Optional\n",
        "    \n",
        "    # Training parameters\n",
        "    batch_size: int = 32\n",
        "    learning_rate: float = 1e-3\n",
        "    n_epochs: int = 100\n",
        "    patience: int = 10\n",
        "    \n",
        "    # Model architecture\n",
        "    n_filters: int = 64\n",
        "    n_depths: int = 3\n",
        "    dropout_rate: float = 0.2\n",
        "\n",
        "# Initialize configuration\n",
        "CFG = Config()\n",
        "print(f\"Configuration loaded: T={CFG.T}, Variables={len(CFG.variables)}\")\n",
        "print(f\"Training period: {CFG.train_start} to {CFG.train_end}\")\n",
        "print(f\"Validation period: {CFG.val_start} to {CFG.val_end}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchmetrics\n",
        "\n",
        "# Try optional imports\n",
        "try:\n",
        "    import xarray as xr\n",
        "    XARRAY_AVAILABLE = True\n",
        "    print(\"âœ“ xarray available\")\n",
        "except ImportError:\n",
        "    XARRAY_AVAILABLE = False\n",
        "    print(\"âš  xarray not available\")\n",
        "\n",
        "try:\n",
        "    import zarr\n",
        "    ZARR_AVAILABLE = True\n",
        "    print(\"âœ“ zarr available\")\n",
        "except ImportError:\n",
        "    ZARR_AVAILABLE = False\n",
        "    print(\"âš  zarr not available\")\n",
        "\n",
        "try:\n",
        "    import dask\n",
        "    DASK_AVAILABLE = True\n",
        "    print(\"âœ“ dask available\")\n",
        "except ImportError:\n",
        "    DASK_AVAILABLE = False\n",
        "    print(\"âš  dask not available\")\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(CFG.seed)\n",
        "np.random.seed(CFG.seed)\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Mixed precision support\n",
        "try:\n",
        "    from torch.cuda.amp import autocast, GradScaler\n",
        "    MIXED_PRECISION = True\n",
        "    print(\"âœ“ Mixed precision available\")\n",
        "except ImportError:\n",
        "    MIXED_PRECISION = False\n",
        "    print(\"âš  Mixed precision not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Loader\n",
        "class SpatiotemporalDataset(Dataset):\n",
        "    \"\"\"Dataset for spatiotemporal fire prediction\"\"\"\n",
        "    \n",
        "    def __init__(self, data_dict, target_dict, mask_dict=None, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dict: {variable: [T, H, W, C]} arrays\n",
        "            target_dict: {target: [H, W]} arrays  \n",
        "            mask_dict: {variable: [T, H, W]} boolean masks for missing data\n",
        "            transform: Optional transform\n",
        "        \"\"\"\n",
        "        self.data_dict = data_dict\n",
        "        self.target_dict = target_dict\n",
        "        self.mask_dict = mask_dict or {}\n",
        "        self.transform = transform\n",
        "        \n",
        "        # Get dimensions\n",
        "        self.T, self.H, self.W = next(iter(data_dict.values())).shape[:3]\n",
        "        self.n_samples = self.H * self.W\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Convert flat index to 2D coordinates\n",
        "        h = idx // self.W\n",
        "        w = idx % self.W\n",
        "        \n",
        "        # Extract data for this spatial location\n",
        "        sample = {}\n",
        "        \n",
        "        # Stack time as channels: [T*C] -> [T*C, H, W]\n",
        "        for var, data in self.data_dict.items():\n",
        "            # data shape: [T, H, W, C] -> [T*C, H, W]\n",
        "            var_data = data[:, h, w, :].reshape(-1)  # [T*C]\n",
        "            sample[var] = torch.FloatTensor(var_data)\n",
        "        \n",
        "        # Add target\n",
        "        for target, target_data in self.target_dict.items():\n",
        "            sample[target] = torch.FloatTensor([target_data[h, w]])\n",
        "        \n",
        "        # Add masks if available\n",
        "        for var, mask in self.mask_dict.items():\n",
        "            var_mask = mask[:, h, w].reshape(-1)  # [T]\n",
        "            sample[f'{var}_mask'] = torch.BoolTensor(var_mask)\n",
        "        \n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "            \n",
        "        return sample\n",
        "\n",
        "def load_spatiotemporal_data(data_path, variables, static_variables=None):\n",
        "    \"\"\"Load spatiotemporal data from zarr/netCDF file\"\"\"\n",
        "    print(f\"Loading data from: {data_path}\")\n",
        "    \n",
        "    # This is a placeholder - adapt based on your data format\n",
        "    # For now, create dummy data for demonstration\n",
        "    print(\"âš  Using dummy data - replace with actual data loading\")\n",
        "    \n",
        "    # Dummy dimensions (0.25Â° grid)\n",
        "    H, W = 180, 360  # Approximate global 0.25Â° grid\n",
        "    T = CFG.T\n",
        "    n_days = 1000  # Total days\n",
        "    \n",
        "    # Create dummy data\n",
        "    data_dict = {}\n",
        "    for var in variables:\n",
        "        # Shape: [T, H, W, 1] for each variable\n",
        "        data_dict[var] = np.random.randn(n_days, H, W, 1).astype(np.float32)\n",
        "    \n",
        "    # Add static variables\n",
        "    if static_variables:\n",
        "        for var in static_variables:\n",
        "            data_dict[var] = np.random.randn(n_days, H, W, 1).astype(np.float32)\n",
        "    \n",
        "    # Create dummy target (fire counts)\n",
        "    target_dict = {\n",
        "        'fire_count': np.random.poisson(0.1, (n_days, H, W)).astype(np.float32)\n",
        "    }\n",
        "    \n",
        "    # Create date array\n",
        "    dates = pd.date_range('2020-01-01', periods=n_days, freq='D')\n",
        "    \n",
        "    print(f\"Data shape: {n_days} days, {H}x{W} grid\")\n",
        "    print(f\"Variables: {list(data_dict.keys())}\")\n",
        "    print(f\"Target: {list(target_dict.keys())}\")\n",
        "    \n",
        "    return data_dict, target_dict, dates\n",
        "\n",
        "# Load data\n",
        "data_dict, target_dict, dates = load_spatiotemporal_data(\n",
        "    CFG.data_zarr_or_nc_path, \n",
        "    CFG.variables, \n",
        "    CFG.static_variables\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/Validation Split and Normalization\n",
        "print(\"Creating train/validation split...\")\n",
        "\n",
        "# Convert dates to datetime\n",
        "dates = pd.to_datetime(dates)\n",
        "\n",
        "# Create train/val masks\n",
        "train_mask = (dates >= CFG.train_start) & (dates <= CFG.train_end)\n",
        "val_mask = (dates >= CFG.val_start) & (dates <= CFG.val_end)\n",
        "\n",
        "print(f\"Training period: {dates[train_mask].min()} to {dates[train_mask].max()} ({train_mask.sum()} days)\")\n",
        "print(f\"Validation period: {dates[val_mask].min()} to {dates[val_mask].max()} ({val_mask.sum()} days)\")\n",
        "\n",
        "# Split data\n",
        "train_data = {var: data[train_mask] for var, data in data_dict.items()}\n",
        "val_data = {var: data[val_mask] for var, data in data_dict.items()}\n",
        "\n",
        "train_target = {target: data[train_mask] for target, data in target_dict.items()}\n",
        "val_target = {target: data[val_mask] for target, data in target_dict.items()}\n",
        "\n",
        "# Normalization using training statistics\n",
        "print(\"Computing normalization statistics...\")\n",
        "scalers = {}\n",
        "for var in CFG.variables + CFG.static_variables:\n",
        "    if var in train_data:\n",
        "        # Compute mean and std across time and space\n",
        "        var_data = train_data[var].reshape(-1, train_data[var].shape[-1])\n",
        "        scalers[var] = {\n",
        "            'mean': np.mean(var_data, axis=0),\n",
        "            'std': np.std(var_data, axis=0) + 1e-8  # Add small epsilon\n",
        "        }\n",
        "        print(f\"  {var}: mean={scalers[var]['mean'][0]:.3f}, std={scalers[var]['std'][0]:.3f}\")\n",
        "\n",
        "# Normalize data\n",
        "def normalize_data(data_dict, scalers):\n",
        "    normalized = {}\n",
        "    for var, data in data_dict.items():\n",
        "        if var in scalers:\n",
        "            normalized[var] = (data - scalers[var]['mean']) / scalers[var]['std']\n",
        "        else:\n",
        "            normalized[var] = data\n",
        "    return normalized\n",
        "\n",
        "train_data_norm = normalize_data(train_data, scalers)\n",
        "val_data_norm = normalize_data(val_data, scalers)\n",
        "\n",
        "print(\"âœ“ Data normalized using training statistics\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# U-Net Model Architecture\n",
        "class SpatiotemporalUNet(nn.Module):\n",
        "    \"\"\"Simple U-Net for spatiotemporal fire prediction\"\"\"\n",
        "    \n",
        "    def __init__(self, input_channels, n_filters=64, n_depths=3, dropout_rate=0.2):\n",
        "        super(SpatiotemporalUNet, self).__init__()\n",
        "        \n",
        "        self.n_filters = n_filters\n",
        "        self.n_depths = n_depths\n",
        "        \n",
        "        # Encoder (downsampling path)\n",
        "        self.encoder = nn.ModuleList()\n",
        "        in_channels = input_channels\n",
        "        \n",
        "        for i in range(n_depths):\n",
        "            out_channels = n_filters * (2 ** i)\n",
        "            self.encoder.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                    nn.BatchNorm2d(out_channels),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "                    nn.BatchNorm2d(out_channels),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Dropout2d(dropout_rate)\n",
        "                )\n",
        "            )\n",
        "            in_channels = out_channels\n",
        "        \n",
        "        # Decoder (upsampling path)\n",
        "        self.decoder = nn.ModuleList()\n",
        "        \n",
        "        for i in range(n_depths - 1, 0, -1):\n",
        "            in_channels = n_filters * (2 ** i)\n",
        "            out_channels = n_filters * (2 ** (i - 1))\n",
        "            self.decoder.append(\n",
        "                nn.Sequential(\n",
        "                    nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
        "                    nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                    nn.BatchNorm2d(out_channels),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "                    nn.BatchNorm2d(out_channels),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Dropout2d(dropout_rate)\n",
        "                )\n",
        "            )\n",
        "        \n",
        "        # Final output layer\n",
        "        self.final_conv = nn.Sequential(\n",
        "            nn.Conv2d(n_filters, 1, kernel_size=1),\n",
        "            nn.Softplus()  # Ensure positive output for count prediction\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        encoder_outputs = []\n",
        "        for i, encoder_block in enumerate(self.encoder):\n",
        "            x = encoder_block(x)\n",
        "            encoder_outputs.append(x)\n",
        "            if i < len(self.encoder) - 1:  # Don't downsample after last encoder\n",
        "                x = nn.functional.max_pool2d(x, 2)\n",
        "        \n",
        "        # Decoder\n",
        "        for i, decoder_block in enumerate(self.decoder):\n",
        "            x = nn.functional.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "            # Skip connection\n",
        "            if i < len(encoder_outputs) - 1:\n",
        "                skip = encoder_outputs[-(i+2)]\n",
        "                x = torch.cat([x, skip], dim=1)\n",
        "            x = decoder_block(x)\n",
        "        \n",
        "        # Final output\n",
        "        x = self.final_conv(x)\n",
        "        return x\n",
        "\n",
        "# Calculate input channels\n",
        "input_channels = CFG.T * len(CFG.variables)\n",
        "if CFG.static_variables:\n",
        "    input_channels += len(CFG.static_variables)\n",
        "\n",
        "print(f\"Model input channels: {input_channels}\")\n",
        "print(f\"Architecture: {CFG.n_depths} depths, {CFG.n_filters} base filters\")\n",
        "\n",
        "# Create model\n",
        "model = SpatiotemporalUNet(\n",
        "    input_channels=input_channels,\n",
        "    n_filters=CFG.n_filters,\n",
        "    n_depths=CFG.n_depths,\n",
        "    dropout_rate=CFG.dropout_rate\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M parameters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss Functions\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
        "    \n",
        "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        \n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = nn.functional.mse_loss(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "        \n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "class TweedieLoss(nn.Module):\n",
        "    \"\"\"Tweedie Loss for count data (Poisson-like)\"\"\"\n",
        "    \n",
        "    def __init__(self, p=1.5):\n",
        "        super(TweedieLoss, self).__init__()\n",
        "        self.p = p\n",
        "        \n",
        "    def forward(self, pred, target):\n",
        "        # Tweedie deviance\n",
        "        if self.p == 1:  # Poisson\n",
        "            return torch.mean(pred - target * torch.log(pred + 1e-8))\n",
        "        elif self.p == 2:  # Gamma\n",
        "            return torch.mean(target / (pred + 1e-8) + torch.log(pred + 1e-8))\n",
        "        else:  # General Tweedie\n",
        "            return torch.mean(\n",
        "                (target * torch.pow(pred, 1 - self.p)) / (1 - self.p) - \n",
        "                torch.pow(pred, 2 - self.p) / (2 - self.p)\n",
        "            )\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.MSELoss()  # Start with MSE, can switch to others\n",
        "# criterion = FocalLoss(alpha=1, gamma=2)  # For class imbalance\n",
        "# criterion = TweedieLoss(p=1.5)  # For count data\n",
        "\n",
        "print(f\"Loss function: {criterion.__class__.__name__}\")\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=CFG.learning_rate, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG.n_epochs)\n",
        "\n",
        "# Mixed precision scaler\n",
        "if MIXED_PRECISION:\n",
        "    scaler = GradScaler()\n",
        "    print(\"âœ“ Mixed precision enabled\")\n",
        "else:\n",
        "    scaler = None\n",
        "    print(\"âš  Mixed precision not available\")\n",
        "\n",
        "print(f\"Optimizer: AdamW (lr={CFG.learning_rate})\")\n",
        "print(f\"Scheduler: CosineAnnealingLR\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device, scaler=None):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    n_batches = 0\n",
        "    \n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Prepare input (stack all variables as channels)\n",
        "        inputs = []\n",
        "        for var in CFG.variables + CFG.static_variables:\n",
        "            if var in batch:\n",
        "                inputs.append(batch[var])\n",
        "        \n",
        "        if not inputs:\n",
        "            continue\n",
        "            \n",
        "        x = torch.stack(inputs, dim=1).to(device)  # [B, T*C, H, W]\n",
        "        y = batch['fire_count'].to(device)\n",
        "        \n",
        "        if scaler is not None:\n",
        "            with autocast():\n",
        "                outputs = model(x)\n",
        "                loss = criterion(outputs.squeeze(), y)\n",
        "            \n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs.squeeze(), y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "    \n",
        "    return total_loss / max(n_batches, 1)\n",
        "\n",
        "def validate_epoch(model, dataloader, criterion, device):\n",
        "    \"\"\"Validate for one epoch\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    n_batches = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            # Prepare input\n",
        "            inputs = []\n",
        "            for var in CFG.variables + CFG.static_variables:\n",
        "                if var in batch:\n",
        "                    inputs.append(batch[var])\n",
        "            \n",
        "            if not inputs:\n",
        "                continue\n",
        "                \n",
        "            x = torch.stack(inputs, dim=1).to(device)\n",
        "            y = batch['fire_count'].to(device)\n",
        "            \n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs.squeeze(), y)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            n_batches += 1\n",
        "    \n",
        "    return total_loss / max(n_batches, 1)\n",
        "\n",
        "# Create dummy dataloaders (replace with actual data)\n",
        "print(\"Creating dummy dataloaders...\")\n",
        "print(\"âš  Replace with actual SpatiotemporalDataset implementation\")\n",
        "\n",
        "# For now, create simple dummy data\n",
        "dummy_train_loader = DataLoader(\n",
        "    range(100),  # Dummy dataset\n",
        "    batch_size=CFG.batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "dummy_val_loader = DataLoader(\n",
        "    range(50),  # Dummy dataset\n",
        "    batch_size=CFG.batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"Training batches: {len(dummy_train_loader)}\")\n",
        "print(f\"Validation batches: {len(dummy_val_loader)}\")\n",
        "print(\"âš  Note: Replace dummy dataloaders with actual SpatiotemporalDataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training with Early Stopping\n",
        "print(\"Starting training...\")\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "best_model_state = None\n",
        "\n",
        "for epoch in range(CFG.n_epochs):\n",
        "    # Training\n",
        "    train_loss = train_epoch(model, dummy_train_loader, optimizer, criterion, device, scaler)\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "    # Validation\n",
        "    val_loss = validate_epoch(model, dummy_val_loader, criterion, device)\n",
        "    val_losses.append(val_loss)\n",
        "    \n",
        "    # Learning rate scheduling\n",
        "    scheduler.step()\n",
        "    \n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_model_state = model.state_dict().copy()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    \n",
        "    # Print progress\n",
        "    if epoch % 10 == 0 or epoch == CFG.n_epochs - 1:\n",
        "        print(f\"Epoch {epoch:3d}/{CFG.n_epochs}: \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
        "              f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "    \n",
        "    # Early stopping\n",
        "    if patience_counter >= CFG.patience:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "# Load best model\n",
        "if best_model_state is not None:\n",
        "    model.load_state_dict(best_model_state)\n",
        "    print(\"âœ“ Loaded best model weights\")\n",
        "\n",
        "print(f\"Training completed. Best validation loss: {best_val_loss:.4f}\")\n",
        "print(f\"Total epochs: {len(train_losses)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "print(\"Evaluating model...\")\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss', alpha=0.8)\n",
        "plt.plot(val_losses, label='Validation Loss', alpha=0.8)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Curves')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(optimizer.param_groups[0]['lr'] * np.ones(len(train_losses)), label='Learning Rate')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title('Learning Rate Schedule')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Model evaluation metrics\n",
        "print(f\"\\nðŸ“Š TRAINING SUMMARY:\")\n",
        "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
        "print(f\"Total Epochs: {len(train_losses)}\")\n",
        "print(f\"Early Stopping: {'Yes' if patience_counter >= CFG.patience else 'No'}\")\n",
        "\n",
        "# Model size and complexity\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nðŸ”§ MODEL COMPLEXITY:\")\n",
        "print(f\"Total Parameters: {total_params:,}\")\n",
        "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
        "print(f\"Model Size: {total_params / 1e6:.2f}M parameters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference Function\n",
        "def predict_next_day(model, recent_data, device, scalers=None):\n",
        "    \"\"\"\n",
        "    Predict fire count for next day given recent T days of data.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained SpatiotemporalUNet model\n",
        "        recent_data: Dict with recent T days of data {var: [T, H, W, C]}\n",
        "        device: torch device\n",
        "        scalers: Optional normalization scalers\n",
        "    \n",
        "    Returns:\n",
        "        np.array: Predicted fire counts [H, W]\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Normalize data if scalers provided\n",
        "        if scalers:\n",
        "            normalized_data = {}\n",
        "            for var, data in recent_data.items():\n",
        "                if var in scalers:\n",
        "                    normalized_data[var] = (data - scalers[var]['mean']) / scalers[var]['std']\n",
        "                else:\n",
        "                    normalized_data[var] = data\n",
        "        else:\n",
        "            normalized_data = recent_data\n",
        "        \n",
        "        # Stack variables as channels\n",
        "        inputs = []\n",
        "        for var in CFG.variables + CFG.static_variables:\n",
        "            if var in normalized_data:\n",
        "                inputs.append(torch.FloatTensor(normalized_data[var]))\n",
        "        \n",
        "        if not inputs:\n",
        "            raise ValueError(\"No input variables found\")\n",
        "        \n",
        "        # Reshape to [1, T*C, H, W] for batch processing\n",
        "        x = torch.stack(inputs, dim=1).unsqueeze(0).to(device)\n",
        "        \n",
        "        # Predict\n",
        "        outputs = model(x)\n",
        "        \n",
        "        # Convert back to numpy\n",
        "        predictions = outputs.squeeze().cpu().numpy()\n",
        "        \n",
        "        return predictions\n",
        "\n",
        "# Example usage (commented out)\n",
        "# recent_data = {\n",
        "#     'tmin': np.random.randn(CFG.T, 180, 360, 1),\n",
        "#     'tmax': np.random.randn(CFG.T, 180, 360, 1),\n",
        "#     # ... other variables\n",
        "# }\n",
        "# predictions = predict_next_day(model, recent_data, device, scalers)\n",
        "# print(f\"Prediction shape: {predictions.shape}\")\n",
        "# print(f\"Prediction range: {predictions.min():.3f} to {predictions.max():.3f}\")\n",
        "\n",
        "print(\"âœ“ Inference function created: predict_next_day()\")\n",
        "print(\"Usage: predictions = predict_next_day(model, recent_data_dict, device, scalers)\")\n",
        "\n",
        "# Save model (optional)\n",
        "# torch.save(model.state_dict(), 'spatiotemporal_unet_model.pth')\n",
        "# print(\"âœ“ Model saved to spatiotemporal_unet_model.pth\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
