{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AutoML-Fire: Two-Stage Baseline (Notebook-first)\n",
        "\n",
        "This notebook implements a two-stage AutoML approach for fire prediction:\n",
        "1. **Stage 1**: Binary classification to predict fire occurrence (y > 0)\n",
        "2. **Stage 2**: Regression to predict fire intensity on fire days only\n",
        "\n",
        "The approach uses time-aware cross-validation to prevent data leakage and includes comprehensive feature engineering with rolling statistics and lag features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Configuration for AutoML two-stage baseline\"\"\"\n",
        "    # Data paths\n",
        "    data_csv_path: str = \"data/combined_top30_dataset.csv\"  # Placeholder - update with your path\n",
        "    \n",
        "    # Column specifications\n",
        "    date_col: str = \"date\"\n",
        "    target_col: str = \"fire_count\"\n",
        "    group_cols: Tuple[str, str] = (\"cluster\", \"grid_id\")\n",
        "    static_cols: List[str] = (\"elevation\", \"slope\", \"aspect\", \"landcover\")  # Allow absent\n",
        "    drop_cols: List[str] = (\"lat\", \"lon\")  # Optional columns to drop\n",
        "    \n",
        "    # Cross-validation\n",
        "    n_splits: int = 5\n",
        "    seed: int = 42\n",
        "    \n",
        "    # Hyperparameter optimization\n",
        "    use_optuna: bool = True\n",
        "    n_trials: int = 40\n",
        "\n",
        "# Initialize configuration\n",
        "CFG = Config()\n",
        "print(f\"Configuration loaded: {CFG.n_splits} CV splits, Optuna: {CFG.use_optuna}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports with safe fallbacks\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, f1_score, \n",
        "    mean_absolute_error, mean_squared_error, r2_score\n",
        ")\n",
        "\n",
        "# Try LightGBM first, fallback to HistGradientBoosting\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    from lightgbm import LGBMClassifier, LGBMRegressor\n",
        "    LIGHTGBM_AVAILABLE = True\n",
        "    print(\"âœ“ LightGBM available\")\n",
        "except ImportError:\n",
        "    LIGHTGBM_AVAILABLE = False\n",
        "    print(\"âš  LightGBM not available, using HistGradientBoosting\")\n",
        "\n",
        "if not LIGHTGBM_AVAILABLE:\n",
        "    from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
        "\n",
        "# Try Optuna for hyperparameter optimization\n",
        "try:\n",
        "    import optuna\n",
        "    OPTUNA_AVAILABLE = True\n",
        "    print(\"âœ“ Optuna available\")\n",
        "except ImportError:\n",
        "    OPTUNA_AVAILABLE = False\n",
        "    print(\"âš  Optuna not available, using default parameters\")\n",
        "\n",
        "print(f\"Available models: LightGBM={LIGHTGBM_AVAILABLE}, Optuna={OPTUNA_AVAILABLE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare data\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv(CFG.data_csv_path)\n",
        "\n",
        "# Parse date column\n",
        "df[CFG.date_col] = pd.to_datetime(df[CFG.date_col])\n",
        "df = df.sort_values(CFG.date_col).reset_index(drop=True)\n",
        "\n",
        "print(f\"Data shape: {df.shape}\")\n",
        "print(f\"Date range: {df[CFG.date_col].min()} to {df[CFG.date_col].max()}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values per column:\")\n",
        "missing_counts = df.isnull().sum()\n",
        "print(missing_counts[missing_counts > 0])\n",
        "\n",
        "# Check for required columns\n",
        "required_cols = [CFG.date_col, CFG.target_col] + list(CFG.group_cols)\n",
        "missing_required = [col for col in required_cols if col not in df.columns]\n",
        "if missing_required:\n",
        "    raise ValueError(f\"Missing required columns: {missing_required}\")\n",
        "\n",
        "# Check for static columns\n",
        "missing_static = [col for col in CFG.static_cols if col not in df.columns]\n",
        "if missing_static:\n",
        "    print(f\"âš  Warning: Missing static columns: {missing_static}\")\n",
        "\n",
        "print(f\"\\nTarget distribution:\")\n",
        "print(df[CFG.target_col].describe())\n",
        "print(f\"Fire days (y > 0): {(df[CFG.target_col] > 0).sum()} / {len(df)} ({100*(df[CFG.target_col] > 0).mean():.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering\n",
        "print(\"Creating features...\")\n",
        "\n",
        "# Create binary classification target\n",
        "df['y_cls'] = (df[CFG.target_col] > 0).astype(int)\n",
        "print(f\"Classification target: {df['y_cls'].mean():.3f} positive rate\")\n",
        "\n",
        "# Create regression target (log1p for positive values)\n",
        "df['y_reg'] = np.log1p(df[CFG.target_col])\n",
        "print(f\"Regression target (log1p): mean={df['y_reg'].mean():.3f}, std={df['y_reg'].std():.3f}\")\n",
        "\n",
        "# Calendar features\n",
        "df['month'] = df[CFG.date_col].dt.month\n",
        "df['day_of_year'] = df[CFG.date_col].dt.dayofyear\n",
        "\n",
        "# Cyclical encoding for seasonality\n",
        "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "df['day_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
        "df['day_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
        "\n",
        "print(\"âœ“ Calendar features created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create rolling statistics and lag features (prevent leakage)\n",
        "print(\"Creating temporal features...\")\n",
        "\n",
        "# Key meteorological variables for rolling stats\n",
        "meteo_vars = ['humidity', 'windspeed', 'rain', 'soil_moisture', 'tmin', 'tmax', 'ndvi', 'cloudcover']\n",
        "available_meteo = [var for var in meteo_vars if var in df.columns]\n",
        "print(f\"Available meteorological variables: {available_meteo}\")\n",
        "\n",
        "# Rolling windows (past-only to prevent leakage)\n",
        "rolling_windows = [3, 7, 14, 30]\n",
        "lag_days = [1, 2, 3, 7]\n",
        "\n",
        "# Group by spatial units to prevent cross-contamination\n",
        "group_cols = [col for col in CFG.group_cols if col in df.columns]\n",
        "print(f\"Grouping by: {group_cols}\")\n",
        "\n",
        "for var in available_meteo:\n",
        "    if var in df.columns:\n",
        "        # Rolling statistics (past-only)\n",
        "        for window in rolling_windows:\n",
        "            df[f'{var}_roll{window}_mean'] = df.groupby(group_cols)[var].shift(1).rolling(window, min_periods=1).mean()\n",
        "            df[f'{var}_roll{window}_std'] = df.groupby(group_cols)[var].shift(1).rolling(window, min_periods=1).std()\n",
        "            df[f'{var}_roll{window}_max'] = df.groupby(group_cols)[var].shift(1).rolling(window, min_periods=1).max()\n",
        "            df[f'{var}_roll{window}_min'] = df.groupby(group_cols)[var].shift(1).rolling(window, min_periods=1).min()\n",
        "        \n",
        "        # Lag features\n",
        "        for lag in lag_days:\n",
        "            df[f'{var}_lag{lag}'] = df.groupby(group_cols)[var].shift(lag)\n",
        "\n",
        "# Target lag features\n",
        "for lag in lag_days:\n",
        "    df[f'{CFG.target_col}_lag{lag}'] = df.groupby(group_cols)[CFG.target_col].shift(lag)\n",
        "\n",
        "print(\"âœ“ Temporal features created (rolling stats and lags)\")\n",
        "print(f\"Total features created: {len([col for col in df.columns if any(x in col for x in ['_roll', '_lag'])])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional fire danger indices (lightweight approximations)\n",
        "print(\"Creating fire danger indices...\")\n",
        "\n",
        "# Simple FWI-like components if variables available\n",
        "if 'tmax' in df.columns and 'humidity' in df.columns and 'windspeed' in df.columns:\n",
        "    # Temperature component (simplified)\n",
        "    df['temp_component'] = np.maximum(0, df['tmax'] - 20)  # Base temperature\n",
        "    \n",
        "    # Humidity component (inverse relationship)\n",
        "    df['humidity_component'] = np.maximum(0, 100 - df['humidity'])\n",
        "    \n",
        "    # Wind component\n",
        "    df['wind_component'] = df['windspeed'] ** 2\n",
        "    \n",
        "    # Simple fire danger index\n",
        "    df['fire_danger_index'] = (df['temp_component'] * df['humidity_component'] * df['wind_component']) / 1000\n",
        "    print(\"âœ“ Fire danger index created\")\n",
        "else:\n",
        "    print(\"âš  Insufficient variables for fire danger index\")\n",
        "\n",
        "# KBDI-like soil moisture proxy\n",
        "if 'soil_moisture' in df.columns and 'rain' in df.columns:\n",
        "    # Simple drought index (inverse of soil moisture, reduced by rain)\n",
        "    df['drought_index'] = np.maximum(0, 100 - df['soil_moisture'] - df['rain'])\n",
        "    print(\"âœ“ Drought index created\")\n",
        "else:\n",
        "    print(\"âš  Insufficient variables for drought index\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare feature matrix and targets\n",
        "print(\"Preparing feature matrix...\")\n",
        "\n",
        "# Define feature columns (exclude targets, dates, and group IDs)\n",
        "exclude_cols = [CFG.date_col, CFG.target_col, 'y_cls', 'y_reg'] + list(CFG.group_cols)\n",
        "if CFG.drop_cols:\n",
        "    exclude_cols.extend([col for col in CFG.drop_cols if col in df.columns])\n",
        "\n",
        "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
        "print(f\"Feature columns: {len(feature_cols)}\")\n",
        "\n",
        "# Create feature matrix\n",
        "X = df[feature_cols].copy()\n",
        "y_cls = df['y_cls'].copy()\n",
        "y_reg = df['y_reg'].copy()\n",
        "\n",
        "# Handle missing values in features\n",
        "print(f\"Missing values in features: {X.isnull().sum().sum()}\")\n",
        "X = X.fillna(X.median())  # Simple imputation for now\n",
        "\n",
        "print(f\"Final feature matrix shape: {X.shape}\")\n",
        "print(f\"Classification target distribution: {y_cls.value_counts().to_dict()}\")\n",
        "print(f\"Regression target stats: mean={y_reg.mean():.3f}, std={y_reg.std():.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time-aware cross-validation setup\n",
        "print(\"Setting up time-aware cross-validation...\")\n",
        "\n",
        "# Create time series split\n",
        "tscv = TimeSeriesSplit(n_splits=CFG.n_splits, gap=0)\n",
        "print(f\"Time series CV: {CFG.n_splits} splits\")\n",
        "\n",
        "# Store CV results\n",
        "cv_results_cls = []\n",
        "cv_results_reg = []\n",
        "oof_probs_cls = np.zeros(len(X))\n",
        "oof_preds_reg = np.zeros(len(X))\n",
        "\n",
        "print(\"âœ“ Cross-validation setup complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stage 1: Binary Classification\n",
        "print(\"Training Stage 1: Binary Classification...\")\n",
        "\n",
        "# Define classifier with safe fallbacks\n",
        "if LIGHTGBM_AVAILABLE:\n",
        "    clf = LGBMClassifier(\n",
        "        objective='binary',\n",
        "        class_weight='balanced',\n",
        "        random_state=CFG.seed,\n",
        "        verbose=-1,\n",
        "        n_estimators=100\n",
        "    )\n",
        "    print(\"Using LightGBM Classifier\")\n",
        "else:\n",
        "    clf = HistGradientBoostingClassifier(\n",
        "        random_state=CFG.seed,\n",
        "        max_iter=100\n",
        "    )\n",
        "    print(\"Using HistGradientBoosting Classifier\")\n",
        "\n",
        "# Cross-validation for classification\n",
        "for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
        "    print(f\"Fold {fold + 1}/{CFG.n_splits}\")\n",
        "    \n",
        "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train, y_val = y_cls.iloc[train_idx], y_cls.iloc[val_idx]\n",
        "    \n",
        "    # Train classifier\n",
        "    clf.fit(X_train, y_train)\n",
        "    \n",
        "    # Predict probabilities\n",
        "    y_pred_proba = clf.predict_proba(X_val)[:, 1]\n",
        "    oof_probs_cls[val_idx] = y_pred_proba\n",
        "    \n",
        "    # Calculate metrics\n",
        "    y_pred_binary = (y_pred_proba > 0.5).astype(int)\n",
        "    \n",
        "    auc = roc_auc_score(y_val, y_pred_proba)\n",
        "    ap = average_precision_score(y_val, y_pred_proba)\n",
        "    f1 = f1_score(y_val, y_pred_binary)\n",
        "    \n",
        "    cv_results_cls.append({\n",
        "        'fold': fold + 1,\n",
        "        'auc': auc,\n",
        "        'ap': ap,\n",
        "        'f1': f1\n",
        "    })\n",
        "    \n",
        "    print(f\"  AUC: {auc:.3f}, AP: {ap:.3f}, F1: {f1:.3f}\")\n",
        "\n",
        "# Overall classification metrics\n",
        "overall_auc = roc_auc_score(y_cls, oof_probs_cls)\n",
        "overall_ap = average_precision_score(y_cls, oof_probs_cls)\n",
        "overall_f1 = f1_score(y_cls, (oof_probs_cls > 0.5).astype(int))\n",
        "\n",
        "print(f\"\\nOverall Classification Results:\")\n",
        "print(f\"AUC: {overall_auc:.3f}\")\n",
        "print(f\"Average Precision: {overall_ap:.3f}\")\n",
        "print(f\"F1 Score: {overall_f1:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stage 2: Regression (on fire days only)\n",
        "print(\"Training Stage 2: Regression...\")\n",
        "\n",
        "# Define regressor with safe fallbacks\n",
        "if LIGHTGBM_AVAILABLE:\n",
        "    reg = LGBMRegressor(\n",
        "        objective='tweedie',  # Good for count data\n",
        "        random_state=CFG.seed,\n",
        "        verbose=-1,\n",
        "        n_estimators=100\n",
        "    )\n",
        "    print(\"Using LightGBM Regressor (Tweedie)\")\n",
        "else:\n",
        "    reg = HistGradientBoostingRegressor(\n",
        "        random_state=CFG.seed,\n",
        "        max_iter=100\n",
        "    )\n",
        "    print(\"Using HistGradientBoosting Regressor\")\n",
        "\n",
        "# Cross-validation for regression (only on positive samples)\n",
        "for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
        "    print(f\"Fold {fold + 1}/{CFG.n_splits}\")\n",
        "    \n",
        "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train, y_val = y_reg.iloc[train_idx], y_reg.iloc[val_idx]\n",
        "    \n",
        "    # Only train on positive samples (fire days)\n",
        "    fire_mask_train = y_cls.iloc[train_idx] == 1\n",
        "    fire_mask_val = y_cls.iloc[val_idx] == 1\n",
        "    \n",
        "    if fire_mask_train.sum() > 0:  # Only if we have fire days in training\n",
        "        X_train_fire = X_train[fire_mask_train]\n",
        "        y_train_fire = y_train[fire_mask_train]\n",
        "        \n",
        "        # Train regressor\n",
        "        reg.fit(X_train_fire, y_train_fire)\n",
        "        \n",
        "        # Predict on all validation samples\n",
        "        y_pred_reg = reg.predict(X_val)\n",
        "        oof_preds_reg[val_idx] = y_pred_reg\n",
        "        \n",
        "        # Calculate metrics on fire days only\n",
        "        if fire_mask_val.sum() > 0:\n",
        "            y_val_fire = y_val[fire_mask_val]\n",
        "            y_pred_fire = y_pred_reg[fire_mask_val]\n",
        "            \n",
        "            mae = mean_absolute_error(y_val_fire, y_pred_fire)\n",
        "            rmse = np.sqrt(mean_squared_error(y_val_fire, y_pred_fire))\n",
        "            r2 = r2_score(y_val_fire, y_pred_fire)\n",
        "            \n",
        "            cv_results_reg.append({\n",
        "                'fold': fold + 1,\n",
        "                'mae': mae,\n",
        "                'rmse': rmse,\n",
        "                'r2': r2,\n",
        "                'n_fire_days': fire_mask_val.sum()\n",
        "            })\n",
        "            \n",
        "            print(f\"  MAE: {mae:.3f}, RMSE: {rmse:.3f}, RÂ²: {r2:.3f} (n_fire={fire_mask_val.sum()})\")\n",
        "        else:\n",
        "            print(f\"  No fire days in validation fold\")\n",
        "    else:\n",
        "        print(f\"  No fire days in training fold\")\n",
        "\n",
        "# Overall regression metrics (on fire days only)\n",
        "fire_mask = y_cls == 1\n",
        "if fire_mask.sum() > 0:\n",
        "    overall_mae = mean_absolute_error(y_reg[fire_mask], oof_preds_reg[fire_mask])\n",
        "    overall_rmse = np.sqrt(mean_squared_error(y_reg[fire_mask], oof_preds_reg[fire_mask]))\n",
        "    overall_r2 = r2_score(y_reg[fire_mask], oof_preds_reg[fire_mask])\n",
        "    \n",
        "    print(f\"\\nOverall Regression Results (fire days only):\")\n",
        "    print(f\"MAE: {overall_mae:.3f}\")\n",
        "    print(f\"RMSE: {overall_rmse:.3f}\")\n",
        "    print(f\"RÂ²: {overall_r2:.3f}\")\n",
        "    print(f\"Fire days evaluated: {fire_mask.sum()}\")\n",
        "else:\n",
        "    print(\"No fire days found for regression evaluation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optuna Hyperparameter Optimization (Optional)\n",
        "if CFG.use_optuna and OPTUNA_AVAILABLE:\n",
        "    print(\"Running Optuna hyperparameter optimization...\")\n",
        "    \n",
        "    def objective_cls(trial):\n",
        "        if LIGHTGBM_AVAILABLE:\n",
        "            params = {\n",
        "                'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
        "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "                'num_leaves': trial.suggest_int('num_leaves', 10, 100),\n",
        "                'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
        "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
        "                'random_state': CFG.seed,\n",
        "                'verbose': -1\n",
        "            }\n",
        "            model = LGBMClassifier(**params)\n",
        "        else:\n",
        "            params = {\n",
        "                'max_iter': trial.suggest_int('max_iter', 50, 200),\n",
        "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "                'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 50),\n",
        "                'random_state': CFG.seed\n",
        "            }\n",
        "            model = HistGradientBoostingClassifier(**params)\n",
        "        \n",
        "        # Simple validation on one fold\n",
        "        train_idx, val_idx = list(tscv.split(X))[0]\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y_cls.iloc[train_idx], y_cls.iloc[val_idx]\n",
        "        \n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
        "        return roc_auc_score(y_val, y_pred_proba)\n",
        "    \n",
        "    # Run optimization\n",
        "    study_cls = optuna.create_study(direction='maximize')\n",
        "    study_cls.optimize(objective_cls, n_trials=CFG.n_trials, show_progress_bar=True)\n",
        "    \n",
        "    print(f\"Best classification params: {study_cls.best_params}\")\n",
        "    print(f\"Best classification AUC: {study_cls.best_value:.3f}\")\n",
        "    \n",
        "    # Update classifier with best params\n",
        "    if LIGHTGBM_AVAILABLE:\n",
        "        clf = LGBMClassifier(**study_cls.best_params, verbose=-1)\n",
        "    else:\n",
        "        clf = HistGradientBoostingClassifier(**study_cls.best_params)\n",
        "    \n",
        "else:\n",
        "    print(\"Skipping Optuna optimization (disabled or not available)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Training on Full Data\n",
        "print(\"Training final models on full dataset...\")\n",
        "\n",
        "# Train final classifier on all data\n",
        "clf.fit(X, y_cls)\n",
        "print(\"âœ“ Final classifier trained\")\n",
        "\n",
        "# Train final regressor on fire days only\n",
        "fire_mask = y_cls == 1\n",
        "if fire_mask.sum() > 0:\n",
        "    X_fire = X[fire_mask]\n",
        "    y_fire = y_reg[fire_mask]\n",
        "    reg.fit(X_fire, y_fire)\n",
        "    print(f\"âœ“ Final regressor trained on {fire_mask.sum()} fire days\")\n",
        "else:\n",
        "    print(\"âš  No fire days found for regressor training\")\n",
        "\n",
        "# Store trained models in memory\n",
        "trained_classifier = clf\n",
        "trained_regressor = reg\n",
        "print(\"Models saved to memory variables: trained_classifier, trained_regressor\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation Summary\n",
        "print(\"=== EVALUATION SUMMARY ===\")\n",
        "\n",
        "# Classification results\n",
        "print(\"\\nðŸ“Š CLASSIFICATION RESULTS:\")\n",
        "print(f\"Overall AUC: {overall_auc:.3f}\")\n",
        "print(f\"Overall Average Precision: {overall_ap:.3f}\")\n",
        "print(f\"Overall F1 Score: {overall_f1:.3f}\")\n",
        "\n",
        "print(\"\\nPer-fold Classification Results:\")\n",
        "for result in cv_results_cls:\n",
        "    print(f\"  Fold {result['fold']}: AUC={result['auc']:.3f}, AP={result['ap']:.3f}, F1={result['f1']:.3f}\")\n",
        "\n",
        "# Regression results\n",
        "if cv_results_reg:\n",
        "    print(f\"\\nðŸ“Š REGRESSION RESULTS (fire days only):\")\n",
        "    print(f\"Overall MAE: {overall_mae:.3f}\")\n",
        "    print(f\"Overall RMSE: {overall_rmse:.3f}\")\n",
        "    print(f\"Overall RÂ²: {overall_r2:.3f}\")\n",
        "    \n",
        "    print(\"\\nPer-fold Regression Results:\")\n",
        "    for result in cv_results_reg:\n",
        "        print(f\"  Fold {result['fold']}: MAE={result['mae']:.3f}, RMSE={result['rmse']:.3f}, RÂ²={result['r2']:.3f} (n_fire={result['n_fire_days']})\")\n",
        "\n",
        "# Two-stage combined predictions\n",
        "print(f\"\\nðŸ”— TWO-STAGE COMBINED PREDICTIONS:\")\n",
        "expected_counts = oof_probs_cls * np.expm1(oof_preds_reg)  # p_fire * exp(y_reg) = expected count\n",
        "print(f\"Expected count range: {expected_counts.min():.3f} to {expected_counts.max():.3f}\")\n",
        "print(f\"Mean expected count: {expected_counts.mean():.3f}\")\n",
        "\n",
        "# Compare with actual\n",
        "actual_counts = df[CFG.target_col].values\n",
        "print(f\"Actual count range: {actual_counts.min():.3f} to {actual_counts.max():.3f}\")\n",
        "print(f\"Mean actual count: {actual_counts.mean():.3f}\")\n",
        "\n",
        "# Overall MAE on expected counts\n",
        "overall_mae_combined = mean_absolute_error(actual_counts, expected_counts)\n",
        "print(f\"Combined MAE (expected vs actual): {overall_mae_combined:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference Helper Function\n",
        "def predict_next_day(df_recent_window_per_grid, trained_clf=None, trained_reg=None):\n",
        "    \"\"\"\n",
        "    Predict fire count for next day given recent window of data per grid.\n",
        "    \n",
        "    Args:\n",
        "        df_recent_window_per_grid: DataFrame with recent data for each grid\n",
        "        trained_clf: Trained classifier (uses global if None)\n",
        "        trained_reg: Trained regressor (uses global if None)\n",
        "    \n",
        "    Returns:\n",
        "        dict: {grid_id: expected_fire_count} for each grid\n",
        "    \"\"\"\n",
        "    if trained_clf is None:\n",
        "        trained_clf = trained_classifier\n",
        "    if trained_reg is None:\n",
        "        trained_reg = trained_regressor\n",
        "    \n",
        "    # Ensure we have the same feature columns as training\n",
        "    feature_cols = [col for col in df_recent_window_per_grid.columns \n",
        "                   if col not in [CFG.date_col, CFG.target_col, 'y_cls', 'y_reg'] + list(CFG.group_cols)]\n",
        "    \n",
        "    # Handle missing values\n",
        "    X_pred = df_recent_window_per_grid[feature_cols].fillna(df_recent_window_per_grid[feature_cols].median())\n",
        "    \n",
        "    # Stage 1: Predict fire probability\n",
        "    p_fire = trained_clf.predict_proba(X_pred)[:, 1]\n",
        "    \n",
        "    # Stage 2: Predict fire intensity (log1p scale)\n",
        "    y_reg_pred = trained_reg.predict(X_pred)\n",
        "    \n",
        "    # Combine: expected count = p_fire * exp(y_reg_pred)\n",
        "    expected_counts = p_fire * np.expm1(y_reg_pred)\n",
        "    \n",
        "    # Return as dictionary\n",
        "    grid_ids = df_recent_window_per_grid[CFG.group_cols[1]] if CFG.group_cols[1] in df_recent_window_per_grid.columns else range(len(expected_counts))\n",
        "    return dict(zip(grid_ids, expected_counts))\n",
        "\n",
        "# Example usage (commented out)\n",
        "# recent_data = df.tail(100)  # Last 100 days\n",
        "# predictions = predict_next_day(recent_data)\n",
        "# print(f\"Example predictions: {list(predictions.items())[:5]}\")\n",
        "\n",
        "print(\"âœ“ Inference helper function created: predict_next_day()\")\n",
        "print(\"Usage: predictions = predict_next_day(your_recent_data_df)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
