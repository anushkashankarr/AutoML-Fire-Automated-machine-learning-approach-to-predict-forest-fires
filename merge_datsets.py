# -*- coding: utf-8 -*-
"""DataMerging-Forest Fire Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M4ehZ3ehTsgYet2Y32hD8dmwMwDgF9kU
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
data_path = '/content/drive/MyDrive/AutoML-Fire-Exports'

# Load topography CSV (change name if different)
topo = pd.read_csv(f'{data_path}/Raw/topography_ecozones.csv')

print("Before cleaning:", topo.shape)
topo.head()

# Keep only meaningful columns
keep_cols = ['ECO_ID', 'ECO_NAME', 'REALM', 'area_km2', 'elevation', 'slope', 'aspect']
topo_clean = topo[keep_cols].copy()

# Remove duplicates or invalid rows
topo_clean = topo_clean.dropna(subset=['ECO_ID'])
topo_clean = topo_clean.drop_duplicates(subset=['ECO_ID'])

print("After cleaning:", topo_clean.shape)
topo_clean.head()

eco_ids = pd.read_csv(f'{data_path}/Raw/India_WWF_Ecozone_IDs.csv')['ECO_ID'].unique()
india_topo = topo_clean[topo_clean['ECO_ID'].isin(eco_ids)]
# india_topo.to_csv(f'{data_path}/topography_ecozones_india.csv', index=False)
# print("✅ Filtered precisely using Indian ecozone IDs")
india_topo.head(10)

import pandas as pd
import os

raw_path = '/content/drive/MyDrive/AutoML-Fire-Exports/Raw'

files = [
    'burned_area_ecozones.csv',
    'vegetation_ecozones.csv',
    'soilmoisture_ecozones.csv',
    'topography_ecozones.csv',
    'population_roads_ecozones.csv',
    'nightlights_ecozones.csv'
]

for file in files:
    file_path = os.path.join(raw_path, file)
    try:
        df = pd.read_csv(file_path, nrows=5)  # read only top few rows (fast)
        print(f"\n📄 {file} → shape: {df.shape}")
        print(df.columns.tolist())
    except Exception as e:
        print(f"⚠️ Could not read {file}: {e}")

import pandas as pd
import os

raw_path = '/content/drive/MyDrive/AutoML-Fire-Exports/Raw'
clean_path = '/content/drive/MyDrive/AutoML-Fire-Exports/Clean'
os.makedirs(clean_path, exist_ok=True)

def clean_and_save(file, keep_cols, rename_map):
    print(f"\n🧹 Cleaning {file}...")
    df = pd.read_csv(f'{raw_path}/{file}', low_memory=False)
    df = df[[c for c in df.columns if c in keep_cols]].copy()
    df.rename(columns=rename_map, inplace=True)
    df.drop_duplicates(inplace=True)
    df.reset_index(drop=True, inplace=True)
    df.to_csv(f'{clean_path}/{file}', index=False)
    print(f"✅ Saved clean version: {clean_path}/{file} ({df.shape[0]} rows, {df.shape[1]} cols)")
    return df.head(3)

# Cleaning each dataset
clean_and_save('burned_area_ecozones.csv',
               ['ECO_ID','ECO_NAME','REALM','year','month','start','end','sum'],
               {'sum':'burned_area'})

clean_and_save('vegetation_ecozones.csv',
               ['ECO_ID','ECO_NAME','REALM','year','month','start','end','NDVI','EVI'],
               {})

clean_and_save('soilmoisture_ecozones.csv',
               ['ECO_ID','ECO_NAME','REALM','year','month','start','end','soil_moisture','evaporation'],
               {})

clean_and_save('topography_ecozones.csv',
               ['ECO_ID','ECO_NAME','REALM','area_km2','elevation','slope','aspect'],
               {})

clean_and_save('population_roads_ecozones.csv',
               ['ECO_ID','ECO_NAME','REALM','population','road_distance'],
               {})

clean_and_save('nightlights_ecozones.csv',
               ['ECO_ID','ECO_NAME','REALM','year','month','start','end','mean'],
               {'mean':'nightlights'})

for file in files:
    file_path = os.path.join(raw_path, file)
    try:
        df = pd.read_csv(file_path, nrows=5)  # read only top few rows (fast)
        print(f"\n📄 {file} → shape: {df.shape}")
        print(df.columns.tolist())
    except Exception as e:
        print(f"⚠️ Could not read {file}: {e}")

import pandas as pd
import os

filtered_path = '/content/drive/MyDrive/AutoML-Fire-Exports/Clean'
final_path = '/content/drive/MyDrive/AutoML-Fire-Exports/Final'
os.makedirs(final_path, exist_ok=True)

def remove_invalid_ecozones(file):
    df = pd.read_csv(f'{filtered_path}/{file}', low_memory=False)

    # remove invalid ecozones (ECO_ID ≤ 0 or missing)
    df = df[df['ECO_ID'] > 0]
    df = df.dropna(subset=['ECO_ID', 'ECO_NAME', 'REALM'])

    # reset index and save
    df.reset_index(drop=True, inplace=True)
    df.to_csv(f'{final_path}/{file}', index=False)

    print(f"✅ Cleaned {file}: {df.shape[0]} rows, {df.shape[1]} cols")
    return df.head(3)

# Clean all filtered datasets
remove_invalid_ecozones('burned_area_ecozones.csv')
remove_invalid_ecozones('vegetation_ecozones.csv')
remove_invalid_ecozones('soilmoisture_ecozones.csv')
remove_invalid_ecozones('topography_ecozones.csv')
remove_invalid_ecozones('population_roads_ecozones.csv')
remove_invalid_ecozones('nightlights_ecozones.csv')

import pandas as pd
import os

final_path = '/content/drive/MyDrive/AutoML-Fire-Exports/Final'

files = [
    'burned_area_ecozones.csv',
    'vegetation_ecozones.csv',
    'soilmoisture_ecozones.csv',
    'topography_ecozones.csv',
    'population_roads_ecozones.csv',
    'nightlights_ecozones.csv'
]

for f in files:
    df = pd.read_csv(f'{final_path}/{f}', nrows=5, low_memory=False)
    print(f"📄 {f} → shape: {df.shape}")
    print(list(df.columns))
    print("-" * 100)

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import gc
import os

final_path = '/content/drive/MyDrive/AutoML-Fire-Exports/Final'
output_path = f'{final_path}/merged_india_ecozones.csv'

def merge_in_chunks(base_df, file_path, on_cols, keep_cols=None, chunk_size=100000):
    """
    Safely merges large CSVs chunk by chunk without crashing RAM.
    Automatically removes duplicate overlapping columns like NDVI_x, NDVI_y.
    """
    import gc, os
    reader = pd.read_csv(file_path, chunksize=chunk_size, low_memory=False)
    chunk_num = 0

    for chunk in reader:
        chunk_num += 1
        print(f"🧩 Merging chunk {chunk_num} from {os.path.basename(file_path)}")

        # keep only the needed columns
        if keep_cols:
            cols_to_keep = list(set(on_cols + keep_cols))
            chunk = chunk[cols_to_keep]

        # remove duplicates of join keys
        chunk.drop_duplicates(subset=on_cols, inplace=True)

        # find overlapping columns other than join keys
        overlap = [c for c in chunk.columns if c in base_df.columns and c not in on_cols]
        if overlap:
            print(f"⚠️ Dropping overlapping columns before merge: {overlap}")
            chunk = chunk.drop(columns=overlap)

        base_df = pd.merge(base_df, chunk, on=on_cols, how='outer')
        print(f"✅ Merged chunk {chunk_num}: shape now {base_df.shape}")

        del chunk
        gc.collect()

    base_df.drop_duplicates(subset=on_cols, inplace=True)
    print(f"✨ Finished merging {os.path.basename(file_path)} → shape: {base_df.shape}")
    return base_df

print("🔥 Loading base dataset (burned_area_ecozones.csv)")
merged = pd.read_csv(f'{final_path}/burned_area_ecozones.csv', low_memory=False)
print("Base shape:", merged.shape)
merged.head(2)

# Vegetation
merged = merge_in_chunks(
    merged,
    f'{final_path}/vegetation_ecozones.csv',
    on_cols=['ECO_ID','year','month'],
    keep_cols=['NDVI','EVI']
)
merged.to_csv(f'{final_path}/merged_after_vegetation.csv', index=False)

# Soil Moisture
merged = merge_in_chunks(
    merged,
    f'{final_path}/soilmoisture_ecozones.csv',
    on_cols=['ECO_ID','year','month'],
    keep_cols=['soil_moisture','evaporation']
)
merged.to_csv(f'{final_path}/merged_after_soil.csv', index=False)

# Nightlights
merged = merge_in_chunks(
    merged,
    f'{final_path}/nightlights_ecozones.csv',
    on_cols=['ECO_ID','year','month'],
    keep_cols=['nightlights']
)
merged.to_csv(f'{final_path}/merged_after_night.csv', index=False)

# Topography (static)
topo = pd.read_csv(f'{final_path}/topography_ecozones.csv', low_memory=False)
merged = merged.merge(
    topo[['ECO_ID','area_km2','aspect','elevation','slope']],
    on='ECO_ID', how='left'
)
print("✅ Topography merged. Shape:", merged.shape)
merged.to_csv(f'{final_path}/merged_after_topo.csv', index=False)

# 🧭 check population + roads dataset
pop_path = f'{final_path}/population_roads_ecozones.csv'
pop = pd.read_csv(pop_path, low_memory=False)

print("📄 population_roads_ecozones.csv → shape:", pop.shape)
print("🔤 Columns:", list(pop.columns))
pop.head(5)

# 🧹 Clean population dataset before merge
pop_clean = (
    pop[['ECO_ID', 'population', 'road_distance']]
    .groupby('ECO_ID', as_index=False)
    .mean(numeric_only=True)   # aggregate duplicate ECO_IDs
)

print("✅ Cleaned population file:", pop_clean.shape)
print(pop_clean.head())

# 🧩 Merge with final merged dataset
merged = merged.merge(pop_clean, on='ECO_ID', how='left')
print("✅ Population merged successfully! Final shape:", merged.shape)

print("🎯 Final merged dataset ready!")
print("Shape:", merged.shape)
print("Columns:", list(merged.columns))
merged.head(10)

"""Sanity check

"""

# reorder
cols = ['ECO_ID','ECO_NAME','REALM','year','month','burned_area','NDVI','EVI',
        'soil_moisture','evaporation','nightlights','population','road_distance',
        'elevation','slope','aspect','area_km2']
merged = merged[cols]

merged.to_csv(output_path, index=False)
print(f"🎯 Final merged dataset saved: {output_path}")
print("Shape:", merged.shape)

import pandas as pd

final_path = '/content/drive/MyDrive/AutoML-Fire-Exports/Final/merged_india_ecozones.csv'
df = pd.read_csv(final_path)

print(df.isna().mean().sort_values(ascending=False).head(10))

# drop columns that are >85% missing
df_clean = df.loc[:, df.isna().mean() < 0.85]

print("✅ Columns kept:", df_clean.columns.tolist())
print("Shape after cleanup:", df_clean.shape)

# fill small gaps
df_clean = df_clean.fillna(df_clean.median(numeric_only=True))

print(df['year'].value_counts().sort_index())

print(df[['burned_area','NDVI','EVI','soil_moisture','nightlights']].corr())

# Load your merged dataset
final_path = '/content/drive/MyDrive/AutoML-Fire-Exports/Final/merged_india_ecozones.csv'
df = pd.read_csv(final_path)

# Print all column names
print("🧾 All Column Names:")
for col in df.columns:
    print(col)

# OR compact single-line view
print("\n🔤 Column list:", list(df.columns))

df.head(100)

import pandas as pd

final_path = '/content/drive/MyDrive/AutoML-Fire-Exports/Final/merged_india_ecozones.csv'
merged = pd.read_csv(final_path)

print("✅ Final file verified!")
print("Shape:", merged.shape)
print("\nColumns:", list(merged.columns))
print("\nMemory usage (MB):", merged.memory_usage(deep=True).sum() / 1e6)
